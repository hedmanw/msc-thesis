\chapter{Discussion}
\section{Inferences}
Interestingly, no participant identifies the projectional editing node selection as a disadvantage of \tooln, and no participant struggles with it. This is in line with the findings of Berger et al. \cite{berger2016mps}, showing that users are proficient inside a projectional editor after a short training session.

Completion times for tasks are , Since a research prototype cannot compete with the user interface design capacities of a large project such as Eclipse, we argue that the required edit operations is a more interesting metric for comparison. With a better interface design, the completion times in \tooln~should decrease. 

(Even though the experiment is inconclusive,)
With respect to defects, respondents still regard manual integration as more error prone -- therefore preferring \tooln.

\section{Threats to Validity}
% Shitty selection from BusyBox and Vim, but no traceability because they don't use pull requests.
This section reports on the threats to validity regarding the controlled experiment.

\subsection{Internal Validity}
\textbf{Choice of programs.} 
Since no domain knowledge was required for conducting the integration tasks, having no previous knowledge of the projects or domains is not a disadvantage.
%Schulze et al. show that the discipline of preprocessor annotations have no effect on program comprehension \cite{schulze2013discipline}.

\textbf{Selection bias.} The participants are randomly assigned to programs and treatments in the Latin square, minimizing selection bias. Additionally, each participant is exposed to both programs, and both treatments.

\textbf{Carryover effects.} Since two tasks are performed, it is possible that carryover effects influence the outcome of the second task, in particular the positive carryover effect of learning. To mitigate any carryover effects, we used a counterbalanced design, where the order of the two tasks are randomized.

\textbf{Social desirability bias.} Students participating in the experiment may have been overly positive in their post-exit questionnaire responses, since they want to encourage and commend the author. This is mitigated by the fact that their answers are confirmed by the industry professionals, who have no similar obligation to the author.

\textbf{Statistical tests.} In the experiment, \anova~is used to determine the statistical significance of the test groups. The impact of the competence of specific subjects is reduced, since \anova~compares group means. TODO: get back to

\subsection{Conclusion Validity}

\textbf{Carryover effects.} A counterbalanced design is used to diminish carryover effects -- but, there could still be effects of asymmetric skill transfer occurring in such a design -- meaning that a particular order of tasks or treatments yield carryover effects, while the other does not. This has not been taken into account in the statistical analysis.

\subsection{External Validity}
Larger programs?
Real programs?
We chose BusyBox and Vim for the subject programs because they are well-known, highly configurable systems, and representative of industrial counterparts \cite{hunsen2016}.
More merging samples. The intentions language is validated on a small number of commits with variant integrations, specifically in the Marlin project. It could be beneficial to sample other suitable candidates that use integrated variability and a pull-based development model. In an effort to make the results more generalizable, we use samples from \vim~and \busybox~in the controlled experiment, with success.
