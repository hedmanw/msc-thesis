\chapter{Methodology}
This chapter describes the data collection of merge samples and the empirical evaluation of the intentions language and intention-based variant integration tool \tooln.

\section{Data Collection}
We choose \marlin~as our initial source of merge examples, because previous work has identified important concepts in the \marlin~ecosystem that we build upon, and because of access to the authors of \cite{stanciulescu2015}, \cite{stanciulescu2016concepts}. Two methods for collecting data from open-source ecosystems have been used: one for gathering a ground truth for how variant integration is achieved through merging, and one for procuring smaller scale authentic integration candidate scenarios. We denote the output of the former method \textit{merge examples} and the output of the latter method \textit{integration scenarios}. The merge examples are used for understanding the context and execution of the variant-integrating merge. They can also be replayed for validation, since they represent the ground truth outcome of the two merge parents. As such, the merge examples serve as an important validation tool. However, since the merge examples are extracted from a three-way merging tool, they are disadvantageous for replaying in \tooln, since three-way information is not available in that environment. To this end, we use the integration scenarios, which are free from three-way contamination, but are instead taken out of their context and are scaled down. They are now described in detail.

\subsection{Collecting Ground Truth Merge Examples}
To identify and validate the integration intentions we analyze variant-related merges and extract common patterns. We begin by retrieving all merges and discard those that merged without conflicts. Merge instances with conflicts signify a syntactic conflict which is an indicator of potential feature integration. In order to investigate further, for each of these merges, we save the state of the entire source tree at the following revisions:  
a) after the merge, i.e. the \textit{result},
b) in the first parent, i.e. the \textit{head}, and
c) in the second parent, the \textit{merge head}. We call this triple of source trees the \textit{related artifacts} of the merge, the relationship between its parts is shown in Figure \ref{intentions:mergespace}. Retaining and inspecting the source trees of the parents allows for insights to be gathered, and in particular the ability to replay the merge in order to reach the known result. Note that the search space, the entire set of merges encompasses \textit{all} merges, that is, merges that occurred in a fork but were subsequently merged into the mainline are also included. We do \textit{not} use the \texttt{-{}-first-parent} option to Git, which would discard such further merges residing in recursive parents, and instead use only the first parent when finding merges. The merges that are included by this broader search scope originate either from ordinary feature merging or from pull requests where the branch of the requester is out of date.%\todo{Figure showing ABL feature integration and Deltabot feature integration.}

When the related artifacts from each merge have been collected, we inspect them and discard merges with any of the following properties: 
a) all conflicts occurred in non-source-code artifacts, b) at least one artifact cannot be compiled, c) there are only whitespace changes between the merge parents, or d) contain some project-specific uninteresting changes (see Section \ref{data-coll-res} for examples of what \textit{uninteresting} entails in practice).

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
        \tikzset{every node/.append style={font=\sffamily}}
        \tikzstyle{linez} = [thick]
        \tikzstyle{commit} = [shape=circle,draw=black, minimum size=1cm, linez]
        \tikzstyle{ancestor} = [draw=gray, dashed]
        \tikzstyle{messager} = [xshift=1cm, yshift=-0.6cm, anchor=west]
        \tikzstyle{messagel} = [xshift=-1cm, yshift=-0.6cm, anchor=east]

        \node[commit, double, label={[messager]Result}] (res) at (0,0) {$m$};
        \node[commit, label={[messager]Head}] (ba) at (0,2) {$p_1$};
        \node[commit, label={[messagel]Merge head}] (re) at (-2,2) {$p_2$};
        \node[commit, ancestor, label={[messager, color=gray]Merge base}] (an) at (0,4) {};
        
        \draw[thick, decoration={brace,mirror,raise=5pt},decorate] (3,-0.5) -- node[right=6pt] {Related artifacts} (3, 2.5);
        
        \draw[thick,->] (-6,4.5) to node[pos=0.5, xshift=0.2cm, rotate=-90] {$time$} (-6,-0.5);
        
        \draw [->, -latex, linez] (res) -- (ba);
        \draw [->, -latex, linez] (res) -- (re);
        \draw [->, -latex, ancestor, linez] (re) -- (an);
        \draw [->, -latex, ancestor, linez] (ba) -- (an);
        % NOTE: There is a bug in tikz with -latex arrows and colors. The arrows should therefore be drawn last, otherwise the colors of the arrowheads can mess up.
        
    \end{tikzpicture}
    \caption{Merge commit context. The merge commit $m$ has two parents, $p_1$ and $p_2$. The entire state of the source tree is gathered, in these three commits. These source sets are denoted \textit{result}, \textit{head}, and \textit{merge head}. This triple of source tree sets constitute the \textit{related artifacts} of the commit $m$.}
    \label{intentions:mergespace}
\end{figure}

\subsection{Creating Authentic Integration Scenarios}
Since the merge examples discussed in the previous section would be replayed without three-way information, and are potentially very large, it is desirable to have a neutral and concentrated basis for task creation, referred to as \textit{integration scenarios}. The contents of integration scenarios are based on variant integration examples from the ecosystem (mainline and forks), with the diff chunks consolidated so that there is not an abundance of code with no changes. Additionally, changes may originate from several different source files, and can have syntax and semantics changed. The structure of the source chunks, and the conflict resolution ground truth is left intact. Since these scenarios are to be used in experiments, the goal is that the result should encompass ca. 50 LOC, as to minimize navigation overhead in the file for the experiment participants.

\section{Completeness Evaluation}
To verify the completeness of the intentions language, we replay all the identified integration commits, and apply intentions to resolve them. The criterion for a successful replay is that the result of the created intention-based resolution is semantically identical (with respect to the C preprocessor) to the ground truth merge resolution. As input to the completeness evaluation, we use the variability-related merge examples from \marlin, which are guaranteed to have well-formed syntax. \change{The completeness evaluation is limited to examples from \marlin, but achieves generality because of the sampling across source-files and forks. This theoretical sampling of merges is employed, as it is impossible to ensure that the evaluated set is representative of the set of all variant integrating merges.}
The objective is to answer the first research question:
\begin{enumerate}[label={Q\arabic*}]
    \setcounter{enumi}{0}
    \item \RQA
\end{enumerate}

\section{Internal Evaluation} \label{m:internal}
\change{As a prestudy for the controlled experiment, we perform an internal evaluation with the three tool developers, exploring how tooling and tasks should be set up for the experiment.} There are two types of tasks: to replay \textit{merge examples}, and to integrate actual forks with the mainline. The characteristics of these tasks are shown in Table \ref{tab:internalchar}. \change{All metrics relate to the inlined model of the programs \cite{lillack2017intentions} (cf. Figure \ref{fig:incline-ast}). \textit{Integrated LOC} signifies the LOC of the source code with inlined variants. \textit{Chunks} is the number of variation blocks from the variants. \textit{Variable LOC} (VLOC) is LOC count of lines under a presence condition containing the literal \texttt{FORK} -- i.e. the LOC count inside the chunks in the inlined model -- in particular, these are the lines that must be manipulated by the participants.} The merge tasks are chosen from the set of merge examples in \marlin, and the forks are chosen based on the findings of St\u{a}nciulescu et al. \cite{stanciulescu2015}. It does not matter whether they are still actively maintained or not, because we set the task at the \head of the fork, and then compare with the latest common ancestor commit \texttt{\#ancestor} between fork and mainline, and devise a resolution strategy for the task.

Each developer performs 3-4 tasks, at least one of each kind. Each task consists of a number of input files from the two variants being integrated, together with a textual description of what the integration goal is. The integration task is performed once in the unstructured two-way merge tool of Eclipse CDT and once in \tooln. The number of edit operations required per task in each editor is saved for analysis, as is the resulting output file. \change{In \tooln, the edit operations are counted once for applying an intention, and for undoing an intention, regardless of the number of nodes selected. In Eclipse, the edit operations are counted once per completed action of insertion, cloning, deletion, and undoing (cf. Berger et al. \cite{berger2016mps}). The output files are compared to the ground truth, and checked for defects introduced. Syntactic differences are allowed, so long as the content is similar semantically with respect to the C preprocessor.}

\begin{table}[ht]
    \centering
    \caption{Internal evaluation task characteristics.}
    \label{tab:internalchar}
    \begin{tabular}{lll|lll}
\hline\hline
\textbf{Name} & \textbf{Type} & \textbf{Files} & \textbf{Integ. LOC} & \textbf{\#chunks} & \textbf{VLOC} \\
%Name & Type & Files& $\pm$ Blocks & $\pm$ \texttt{\#ifdef}s & $\pm$ Lines\\
\hline
08856d9      & Merge     & 1 & 1029 & 16  & 312   \\
17de96a      & Merge     & 1 & 929  & 33  & 392  \\
2daa859      & Merge     & 2 & 4019 & 152 & 1107   \\
3116271      & Merge     & 1 & 146  & 6   & 51    \\
373f3ec      & Merge     & 1 & 2663 & 106 & 749   \\
46f80e8      & Merge     & 1 & 130  & 2   & 2    \\
47c1ea7      & Merge     & 1 & 3400 & 242 & 2391 \\
\hline
esenapaj     & Fork         & 3 & 14909 & 196   & 754   \\
jcrocholl    & Fork         & 1 & 4803  & 40    & 364   \\
STM32        & Fork         & 3 & 7670  & 159   & 966   \\
\hline\hline
    \end{tabular}
\end{table}

\section{Controlled Experiment}
This sections describes the experimental design and setup, together with the \textit{integration scenarios} from \busybox~and \vim, that are the subject programs in the experiment. Before the experiment, we elicit the background of the participants through a questionnaire containing Likert-scale questions about how familiar they are with certain programming languages and merging techniques and tools. The questions are listed in Appendix \ref{a:questionnaires}.
%Designed based on Melo et al \cite{melo2016latin} (also the subsequent paper from 2017). % -- it has really good related work that can be used to show that the preprocessor (and configurations) are often erroneous.

\newcommand{\HA}{Integration in \tooln~is faster than in Eclipse CDT}
\newcommand{\HB}{Integration in \tooln~requires fewer edit operations than in Eclipse CDT}
\newcommand{\HC}{Integration in \tooln~does not lead to more defects than in Eclipse CDT}

\subsection{Objective}
To establish the beneficence of an intention-based integration strategy in general, and INCLINE in particular over two-way merging in Eclipse CDT, we evaluate the tool using subject programs from other sources than \marlin, since \marlin~was used to elicit the set of intentions. The purpose of the controlled experiment is to answer the following two research questions. Additionally, we formulate hypotheses related to Q2:

\begin{enumerate}[label={Q\arabic*}]
    \setcounter{enumi}{1}
    \item \RQB
    \begin{enumerate}[label={H\arabic*}]
        \item \HA.
        \item \HB.
        \item \HC.
    \end{enumerate}
    \item \RQC
\end{enumerate}

To answer these questions, we let participants integrate variants using two different tools, while controlling the confounding factors of developer competence and learning. We measure \ctime, required \eops, and \defx~inserted, and aggregate this as the benefit of a tool. The exploratory question of the differences between processes of intention-based integration over two-way merging has no hypotheses.

\subsection{Treatments}
In order to study the beneficence of intention-based integration, we let each participant solve one task using the ordinary unstructured two-way merge tool inside Eclipse CDT, and one task using \tooln. The two treatment levels are thus \textit{Eclipse CDT}, denoted \texttt{Ecl} and \textit{\tooln}, denoted \inc.

\subsection{Subjects}
\textbf{Participants.}
The experiment is performed with $N=16$ participants: 15 graduate students, and one Ph.D. student. All participants had programming experience, and more than half had more than one year of industrial experience. Regarding integration, all participants were familiar with the Git version control system, and were familiar with ordinary software merging.

\textbf{Programs.} Using the process for collecting authentic variant \textit{integration scenarios}, we prepare two sample programs to be used in the experiment: \po, based on \busybox, and \pt, based on \vim. These two programs are created as condensed versions of the changeset of a particular fork variant, for brevity and comprehension during the experiment. This should be understood as selecting the most suited changes across the files, and placing them in a single file, in order to not waste valuable time for the participants on long blocks without any changes, as to keep the programs brief. Table \ref{method:charac} lists the characteristics of the subject programs, \change{using the same metrics as in the prestudy described in Section \ref{m:internal}. \textit{Integrated LOC} is the line count after the variants have been inlined, \textit{Chunks} is the number of variability blocks, and \textit{Variable LOC (VLOC)} is the LOC count within the chunks.}

We choose \busybox~ and \vim~ as representative subjects because they are highly-configurable open source systems, and have been subjects in previous studies \cite{berger2013study}, \cite{liebig2010preprocessor}, \cite{liebig2011discipline}, \cite{hunsen2016}.

\begin{table}[ht]
    \centering
    \caption{Characteristics of the programs \po~and \pt.}
    \label{method:charac}
    \begin{tabular}{c l l l l}
    \hline
    \hline
        \textbf{Prg} & \textbf{Origin} & \textbf{Integ. LOC} & \textbf{\#chunks} & \textbf{VLOC}\\\hline
        \po & \busybox  & 74 & 8 & 37\\\hline
        \pt & \vim      & 50 & 5 & 32\\
        \hline
        \hline
    \end{tabular}
\end{table}

\subsection{Design}
The experiment is a 2$\times$2 within-subjects counterbalanced Latin square design \cite{box}. Each developer performs two tasks with the treatment order and program order randomized in order to reduce learning effects. Table \ref{method:ls1} shows the base Latin square.
Since developers are assigned at random, no further permutation of the treatments in the 2$\times$2 Latin square is required, since the two other possible combinations would be redundant with respect to program order and treatment. The benefit of a within-subject design is the reduced number of required subjects participating in the experiment and the fact that every subject is exposed to every treatment and program. This however comes at the cost of carryover effects, in this case particularly learning effects. We therefore employ a counterbalanced design, where the order of the tasks is randomized.

\begin{table}[ht]
\centering
\caption{Latin square instance (2$\times$2).}
\label{method:ls1}
\begin{tikzpicture}
    \node[minimum size=1cm,draw=black] (sba) at (0,0) {\inc};
    \node[minimum size=1cm,draw=black] (sbb) at (1,0) {\texttt{Ecl}};
    \node[minimum size=1cm,draw=black] (saa) at (0,1) {\texttt{Ecl}};
    \node[minimum size=1cm,draw=black] (sab) at (1,1) {\inc};
    \node[minimum size=1cm] (ld1) at (-1,1) {\texttt{D1}};
    \node[minimum size=1cm] (ld2) at (-1,0) {\texttt{D2}};
    \node[minimum size=1cm] (lp1) at (0,2) {\texttt{P1}};
    \node[minimum size=1cm] (lp2) at (1,2) {\texttt{P2}};
    
    \node[minimum size=2.15cm,draw=black] (square) at (0.5,0.5) {};
    \node[] (lprog) at (0.5, 2.5) {\textit{programs}};
    \node[rotate=90] (ldevs) at (-1.75, 0.5) {\textit{developers}};
\end{tikzpicture}
\end{table}

\subsection{Execution}
%Before running the actual experiment, we executed a pilot study with the authors of \cite{lillack2017intentions}, in order to assess the presentation of the tasks and the distribution of our tooling.

Before the participants are given their tasks, we present a simple tutorial on integration and variability, and demonstrate how to solve a sample task in each editor. All of tutorials were prerecorded to ensure that all participants receive the same introduction, regardless of which session they attend. Before solving the \tooln~task, the participants are given a warmup task to understand navigation in a projectional editor, and the application of intentions inside the editor. A cheat sheet with examples of intentions is also provided. An instance of a task sheet is shown in Appendix \ref{a:sources}.

During the tasks, the screens of the participants are recorded, which allows measurement of the task completion time. \change{Using the same measurements as in the prestudy, described in Section \ref{m:internal},} both treatment tools are instrumented to output logs with information about keystrokes and menu usage, thereby measuring the required edit operations. Last, the resulting integrated source files created by the participants are collected, to measure the number of defects.

After the experiment, participants fill in a questionnaire with closed and open questions about their perceptions of \tooln~and intention-based integration compared to two-way unstructured integration in Eclipse CDT. The qualitative answers are categorized using open coding \cite{strauss}. The questions are listed in Appendix \ref{a:questionnaires}.

\subsection{Analysis}
The \anova~is used to test the significance of differences between the treatment groups. Cliff's delta is used to calculate the effect size, particularly because it does not require normality.
